{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"},"colab":{"name":"train_fc.ipynb","provenance":[]}},"cells":[{"cell_type":"code","metadata":{"id":"001KbtBtaOVe","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib.gridspec as gridspec\n","import os\n","import argparse"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1e5XXrgQaOWo","colab_type":"code","colab":{}},"source":["def read_data():\n","    from tensorflow.examples.tutorials.mnist import input_data\n","    mnist = input_data.read_data_sets(\"../MNIST_data/\", one_hot=True)\n","    return mnist"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"h-fv8jD9aOXR","colab_type":"code","colab":{}},"source":["def plot(samples):\n","    fig = plt.figure(figsize=(8, 8))\n","    gs = gridspec.GridSpec(8, 8)\n","    gs.update(wspace=0.05, hspace=0.05)\n","\n","    for i, sample in enumerate(samples):\n","        ax = plt.subplot(gs[i])\n","        plt.axis('off')\n","        ax.set_xticklabels([])\n","        ax.set_yticklabels([])\n","        ax.set_aspect('equal')\n","        plt.imshow(sample.reshape(28, 28), cmap='Greys_r')\n","    return fig"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DHPgtwgMaOXt","colab_type":"code","colab":{}},"source":["def train(logdir, batch_size):\n","    from model_fc import discriminator, generator\n","\n","    mnist = read_data()\n","\n","    with tf.variable_scope('placeholder'):\n","        # Raw image\n","        X = tf.placeholder(tf.float32, [None, 784])\n","        tf.summary.image('raw image', tf.reshape(X, [-1, 28, 28, 1]), 3)\n","        # Noise\n","        z = tf.placeholder(tf.float32, [None, 100])  # noise\n","        tf.summary.histogram('Noise', z)\n","\n","    with tf.variable_scope('GAN'):\n","        G = generator(z)\n","\n","        D_real, D_real_logits = discriminator(X, reuse=False)\n","        D_fake, D_fake_logits = discriminator(G, reuse=True)\n","    tf.summary.image('generated image', tf.reshape(G, [-1, 28, 28, 1]), 3)\n","\n","    with tf.variable_scope('Prediction'):\n","        tf.summary.histogram('real', D_real)\n","        tf.summary.histogram('fake', D_fake)\n","\n","    with tf.variable_scope('D_loss'):\n","        # compare D_real_logits with tensor of same shape but containing all 1s.\n","        # compare D_fake_logits with tensor of same shape but containing all 0s.\n","        # We wish D(Y) to approach 1 for real inputs and D(Y) to approach 0 for\n","        # fake inputs.\n","        d_loss_real = tf.reduce_mean(\n","            tf.nn.sigmoid_cross_entropy_with_logits(\n","                logits=D_real_logits, labels=tf.ones_like(D_real_logits)))\n","        d_loss_fake = tf.reduce_mean(\n","            tf.nn.sigmoid_cross_entropy_with_logits(\n","                logits=D_fake_logits, labels=tf.zeros_like(D_fake_logits)))\n","        d_loss = d_loss_real + d_loss_fake\n","\n","        tf.summary.scalar('d_loss_real', d_loss_real)\n","        tf.summary.scalar('d_loss_fake', d_loss_fake)\n","        tf.summary.scalar('d_loss', d_loss)\n","\n","    with tf.name_scope('G_loss'):\n","        #compare D_fake_logits with tensor of same shape but containing all 1s.\n","        #In other words, we wish G(Z) to approach 1.\n","        g_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits\n","                                (logits=D_fake_logits, labels=tf.ones_like(D_fake_logits)))\n","        tf.summary.scalar('g_loss', g_loss)\n","\n","    tvar = tf.trainable_variables()\n","    dvar = [var for var in tvar if 'discriminator' in var.name]\n","    gvar = [var for var in tvar if 'generator' in var.name]\n","\n","    with tf.name_scope('train'):\n","        d_train_step = tf.train.AdamOptimizer().minimize(d_loss, var_list=dvar)\n","        g_train_step = tf.train.AdamOptimizer().minimize(g_loss, var_list=gvar)\n","\n","    sess = tf.Session()\n","    init = tf.global_variables_initializer()\n","    sess.run(init)\n","\n","    merged_summary = tf.summary.merge_all()\n","    writer = tf.summary.FileWriter('tmp/mnist/'+logdir)\n","    writer.add_graph(sess.graph)\n","\n","    num_img = 0\n","    if not os.path.exists('output/'):\n","        os.makedirs('output/')\n","\n","#CellStrat\n","#    for i in range(100000):\n","    for i in range(10000):\n","        batch_X, _ = mnist.train.next_batch(batch_size)\n","        batch_noise = np.random.uniform(-1., 1., [batch_size, 100])\n","\n","        if i % 500 == 0:\n","            samples = sess.run(G, feed_dict={z: np.random.uniform(-1., 1., [64, 100])})\n","            fig = plot(samples)\n","            plt.savefig('output/%s.png' % str(num_img).zfill(3), bbox_inches='tight')\n","            num_img += 1\n","            plt.close(fig)\n","\n","        _, d_loss_print = sess.run([d_train_step, d_loss],\n","                                   feed_dict={X: batch_X, z: batch_noise})\n","\n","        _, g_loss_print = sess.run([g_train_step, g_loss],\n","                                   feed_dict={z: batch_noise})\n","\n","        if i % 100 == 0:\n","            s = sess.run(merged_summary, feed_dict={X: batch_X, z: batch_noise})\n","            writer.add_summary(s, i)\n","            print('epoch:%d g_loss:%f d_loss:%f' % (i, g_loss_print, d_loss_print))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wLDoDETGaOYS","colab_type":"code","colab":{}},"source":["if __name__ == '__main__':\n","    parser = argparse.ArgumentParser(description='Train vanila GAN using fully-connected layers networks')\n","    parser.add_argument('--logdir', type=str, default='1', help='logdir for Tensorboard, give a string')\n","    parser.add_argument('--batch_size', type=int, default=64, help='batch size: give a int')\n","    args = parser.parse_args()\n","\n","    train(logdir=args.logdir, batch_size=args.batch_size)"],"execution_count":0,"outputs":[]}]}